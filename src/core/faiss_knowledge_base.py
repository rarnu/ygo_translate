import json
import re
import hashlib
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
from collections import defaultdict


@dataclass
class TranslationPair:
    """ç¿»è¯‘å¯¹æ•°æ®ç»“æ„"""
    original: str
    translated: str
    metadata: Optional[Dict] = None
    id: str = ""
    category: str = ""
    priority: float = 1.0


class FAISSKnowledgeBase:
    
    def __init__(self, 
                 model_name: str = "all-MiniLM-L6-v2",
                 embedding_dim: int = 384,
                 index_type: str = "flat",
                 max_results: int = 100):
        """
        åˆå§‹åŒ–FAISSçŸ¥è¯†åº“
        
        Args:
            model_name: è¯­ä¹‰æ¨¡å‹åç§°
            embedding_dim: å‘é‡ç»´åº¦
            index_type: ç´¢å¼•ç±»å‹ ("flat", "hnsw", "ivf")
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
        """
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = embedding_dim
        self.index_type = index_type
        self.max_results = max_results
        
        # å­˜å‚¨æ•°æ®
        self.pairs: List[TranslationPair] = []
        self.category_index: Dict[str, List[int]] = defaultdict(list)
        self.id_to_index: Dict[str, int] = {}
        
        # FAISSç´¢å¼•
        self.faiss_index = None
        self.embeddings = None
        self.is_built = False
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self._init_faiss_index()
    
    def _init_faiss_index(self):
        """åˆå§‹åŒ–FAISSç´¢å¼•"""
        if self.index_type == "flat":
            # ç²¾ç¡®æœç´¢ï¼Œæœ€å‡†ç¡®ä½†æœ€æ…¢
            self.faiss_index = faiss.IndexFlatL2(self.embedding_dim)
        elif self.index_type == "hnsw":
            # å±‚æ¬¡åŒ–å°ä¸–ç•Œå›¾ï¼Œå¿«é€Ÿä¸”å‡†ç¡®
            self.faiss_index = faiss.IndexHNSWFlat(self.embedding_dim, 32)
            # HNSWå‚æ•°è°ƒä¼˜
            self.faiss_index.hnsw.efConstruction = 200
            self.faiss_index.hnsw.efSearch = 50
        elif self.index_type == "ivf":
            # å€’æ’æ–‡ä»¶ç´¢å¼•ï¼Œé€‚åˆè¶…å¤§è§„æ¨¡æ•°æ®
            # æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´èšç±»ä¸­å¿ƒæ•°é‡
            nlist = min(100, max(10, len(self.pairs) // 10))
            quantizer = faiss.IndexFlatL2(self.embedding_dim)
            self.faiss_index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, nlist)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {self.index_type}")
    
    def add_translation_pair(self, 
                            original: str, 
                            translated: str, 
                            metadata: Optional[Dict] = None,
                            category: str = "",
                            priority: float = 1.0) -> str:
        """æ·»åŠ ç¿»è¯‘å¯¹åˆ°çŸ¥è¯†åº“"""
        # ç”Ÿæˆå”¯ä¸€ID
        pair_id = hashlib.md5(f"{original}||{translated}".encode()).hexdigest()[:8]
        
        pair = TranslationPair(
            original=original.strip(),
            translated=translated.strip(),
            metadata=metadata or {},
            id=pair_id,
            category=category,
            priority=priority
        )
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        index = len(self.pairs)
        self.pairs.append(pair)
        self.id_to_index[pair_id] = index
        
        # åˆ†ç±»ç´¢å¼•
        if category:
            self.category_index[category].append(index)
        
        # æ ‡è®°éœ€è¦é‡å»ºç´¢å¼•
        self.is_built = False
        
        return pair_id
    
    def batch_add_pairs(self, pairs_data: List[Dict]):
        """æ‰¹é‡æ·»åŠ ç¿»è¯‘å¯¹ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        batch_originals = []
        batch_translated = []
        
        for data in pairs_data:
            original = data['original']
            translated = data['translated']
            metadata = data.get('metadata', {})
            category = data.get('category', '')
            priority = data.get('priority', 1.0)
            
            pair_id = hashlib.md5(f"{original}||{translated}".encode()).hexdigest()[:8]
            
            pair = TranslationPair(
                original=original.strip(),
                translated=translated.strip(),
                metadata=metadata,
                id=pair_id,
                category=category,
                priority=priority
            )
            
            index = len(self.pairs)
            self.pairs.append(pair)
            self.id_to_index[pair_id] = index
            
            if category:
                self.category_index[category].append(index)
            
            batch_originals.append(original)
            batch_translated.append(translated)
        
        # æ‰¹é‡ç¼–ç ï¼ˆæ›´é«˜æ•ˆï¼‰
        if batch_originals:
            new_embeddings = self.embedding_model.encode(
                batch_originals, 
                batch_size=32,
                show_progress_bar=False,
                normalize_embeddings=True
            )
            
            if self.embeddings is None:
                self.embeddings = new_embeddings.astype('float32')
            else:
                self.embeddings = np.vstack([self.embeddings, new_embeddings.astype('float32')])
        
        self.is_built = False

    def find_pair_id(self, original: str, translated: str) -> str | None:
        """æŸ¥æ‰¾ç¿»è¯‘å¯¹ID"""
        pair_id = hashlib.md5(f"{original}||{translated}".encode()).hexdigest()[:8]
        if pair_id not in self.id_to_index:
            return None
        return pair_id

    def find_original_list(self, original: str) -> list[str]:
        """æŸ¥æ‰¾åŸå§‹æ–‡æœ¬å¯¹åº”çš„pair_idåˆ—è¡¨"""
        pair_list = []
        for pair in self.pairs:
            if pair.original == original or pair.translated == original:
                pair_list.append(pair.id)
        return pair_list

    def delete_pair(self, pair_id: str)-> bool:
        """åˆ é™¤ç¿»è¯‘å¯¹"""
        if pair_id not in self.id_to_index:
            return False

        index = self.id_to_index[pair_id]
        pair = self.pairs[index]

        # ä»åˆ†ç±»ç´¢å¼•ä¸­ç§»é™¤
        if pair.category and pair.category in self.category_index:
            category_indices = self.category_index[pair.category]
            if index in category_indices:
                category_indices.remove(index)
            # å¦‚æœç±»åˆ«ä¸ºç©ºï¼Œç§»é™¤è¯¥ç±»åˆ«
            if not category_indices:
                del self.category_index[pair.category]

        # ä»pairsåˆ—è¡¨ä¸­ç§»é™¤
        del self.pairs[index]
        # ä»id_to_indexä¸­ç§»é™¤
        self.id_to_index.pop(pair_id)

        # æ›´æ–°æ‰€æœ‰å¤§äºè¢«åˆ é™¤ç´¢å¼•çš„ç´¢å¼•æ˜ å°„
        new_id_to_index = {}
        for i, updated_pair in enumerate(self.pairs):
            new_id_to_index[updated_pair.id] = i
        self.id_to_index = new_id_to_index

        # æ›´æ–°åˆ†ç±»ç´¢å¼•ä¸­çš„æ‰€æœ‰ç´¢å¼•å€¼
        for category in self.category_index:
            updated_indices = []
            for idx in self.category_index[category]:
                if idx < index:
                    updated_indices.append(idx)
                elif idx > index:
                    updated_indices.append(idx - 1)
            self.category_index[category] = updated_indices

        # ä»embeddingsä¸­ç§»é™¤å¯¹åº”çš„å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.embeddings is not None:
            self.embeddings = np.delete(self.embeddings, index, axis=0)

        # æ ‡è®°éœ€è¦é‡å»ºFAISSç´¢å¼•
        self.is_built = False

        return True


    def build_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        if not self.pairs:
            raise ValueError("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
        
        print(f"æ­£åœ¨æ„å»ºFAISSç´¢å¼•ï¼Œæ•°æ®é‡: {len(self.pairs)}")
        
        # å¦‚æœè¿˜æ²¡æœ‰embeddingsï¼Œåˆ™ç”Ÿæˆ
        if self.embeddings is None:
            print("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
            originals = [pair.original for pair in self.pairs]
            self.embeddings = self.embedding_model.encode(
                originals, 
                batch_size=32,
                show_progress_bar=True,
                normalize_embeddings=True
            ).astype('float32')
        
        # æ„å»ºç´¢å¼•
        if self.index_type == "ivf":
            # IVFç´¢å¼•éœ€è¦å…ˆè®­ç»ƒ
            print("æ­£åœ¨è®­ç»ƒIVFç´¢å¼•...")
            
            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿè®­ç»ƒIVFç´¢å¼•
            n_training_points = len(self.embeddings)
            n_clusters = self.faiss_index.nlist
            
            if n_training_points < n_clusters:
                print(f"âš ï¸  è®­ç»ƒæ•°æ®ä¸è¶³ ({n_training_points} < {n_clusters} èšç±»ä¸­å¿ƒ)")
                print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°Flatç´¢å¼•...")
                # åˆ‡æ¢åˆ°Flatç´¢å¼•
                self.index_type = "flat"
                self._init_faiss_index()
            else:
                self.faiss_index.train(self.embeddings)
        
        print("æ­£åœ¨æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
        self.faiss_index.add(self.embeddings)
        
        self.is_built = True
        print(f"âœ“ FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œç±»å‹: {self.index_type}")
    
    def search_similar(self, 
                      query: str, 
                      top_k: int = 10,
                      category_filter: Optional[str] = None,
                      min_similarity: float = 0.1) -> List[Tuple[TranslationPair, float]]:
        """æœç´¢ç›¸ä¼¼çš„ç¿»è¯‘å¯¹"""
        if not self.pairs:
            print("Debug - çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
            
        if not self.is_built:
            try:
                self.build_index()
            except Exception as e:
                print(f"Debug - æ„å»ºç´¢å¼•å¤±è´¥: {e}")
                return []
        
        # 1. é¦–å…ˆæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        exact_matches = []
        for i, pair in enumerate(self.pairs):
            if pair.original.strip() == query.strip():
                # ç±»åˆ«è¿‡æ»¤
                if category_filter and pair.category != category_filter:
                    continue
                
                # ç²¾ç¡®åŒ¹é…ç»™äºˆæœ€é«˜ç›¸ä¼¼åº¦å’Œä¼˜å…ˆçº§
                exact_matches.append((pair, 10.0 * pair.priority))  # 10.0ä¸ºç²¾ç¡®åŒ¹é…åŸºç¡€åˆ†
        
        # å¦‚æœæœ‰ç²¾ç¡®åŒ¹é…ï¼Œä¼˜å…ˆè¿”å›
        if exact_matches:
            exact_matches.sort(key=lambda x: x[1], reverse=True)
            return exact_matches[:top_k]
        
        # 2. æ²¡æœ‰ç²¾ç¡®åŒ¹é…æ—¶è¿›è¡Œå‘é‡æœç´¢
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedding_model.encode(
            [query], 
            normalize_embeddings=True
        ).astype('float32')
        
        # æœç´¢
        search_k = min(top_k * 3, self.faiss_index.ntotal)  # æœç´¢æ›´å¤šå€™é€‰
        distances, indices = self.faiss_index.search(query_embedding, search_k)
        
        # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆL2è·ç¦»è½¬ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx == -1:  # æ— æ•ˆç´¢å¼•
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ”¹è¿›ç‰ˆï¼šæ›´å¥½çš„è·ç¦»åˆ°ç›¸ä¼¼åº¦è½¬æ¢ï¼‰
            similarity = max(0.0, 1.0 - dist / 2.0)
            
            if similarity < min_similarity:
                continue
            
            pair = self.pairs[idx]
            
            # ç±»åˆ«è¿‡æ»¤
            if category_filter and pair.category != category_filter:
                continue
            
            # åº”ç”¨ä¼˜å…ˆçº§
            adjusted_similarity = similarity * pair.priority
            
            results.append((pair, adjusted_similarity))
        
        # æ’åºå¹¶è¿”å›top_k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def smart_search(self, 
                    query: str,
                    max_tokens: int = 4096,
                    diversity: bool = True) -> List[Tuple[TranslationPair, float]]:
        """æ™ºèƒ½æœç´¢ï¼šè€ƒè™‘tokené™åˆ¶å’Œå¤šæ ·æ€§"""
        print(f"Debug - smart_search è¢«è°ƒç”¨ï¼ŒæŸ¥è¯¢: {query}")
        candidates = self.search_similar(query, top_k=50)
        print(f"Debug - search_similar è¿”å›äº† {len(candidates)} ä¸ªå€™é€‰")
        
        if not candidates:
            print("Debug - æ²¡æœ‰å€™é€‰ç»“æœï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        # æ™ºèƒ½ç­›é€‰
        selected = []
        current_tokens = 0
        used_categories = set()
        
        for pair, similarity in candidates:
            # ä¼°ç®—tokens
            estimated_tokens = self._estimate_tokens(pair.original, pair.translated)
            
            if current_tokens + estimated_tokens > max_tokens:
                continue
            
            # å¤šæ ·æ€§æ§åˆ¶
            if diversity and pair.category and pair.category in used_categories:
                category_count = sum(1 for p, _ in selected if p.category == pair.category)
                if category_count >= 2:  # æ¯ä¸ªç±»åˆ«æœ€å¤š2ä¸ª
                    continue
            
            selected.append((pair, similarity))
            current_tokens += estimated_tokens
            
            if pair.category:
                used_categories.add(pair.category)
            
            if len(selected) >= 10:  # æœ€å¤š10ä¸ª
                break
        
        return selected
    
    def _estimate_tokens(self, original: str, translated: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', translated))
        english_words = len(re.findall(r'\b\w+\b', original + translated))
        punctuation = len(re.findall(r'[^\w\s]', original + translated))
        
        return int(chinese_chars * 1.5 + english_words * 1.3 + punctuation * 0.5)
    
    def save_to_file(self, filepath_prefix: str):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'pairs': [asdict(pair) for pair in self.pairs],
            'category_index': dict(self.category_index),
            'id_to_index': self.id_to_index,
            'config': {
                'model_name': self.embedding_model._modules['0'].auto_model.name_or_path,
                'embedding_dim': self.embedding_dim,
                'index_type': self.index_type,
                'max_results': self.max_results
            }
        }
        
        with open(f"{filepath_prefix}_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.is_built and self.faiss_index:
            faiss.write_index(self.faiss_index, f"{filepath_prefix}.index")
        
        # ä¿å­˜embeddings
        if self.embeddings is not None:
            np.save(f"{filepath_prefix}_embeddings.npy", self.embeddings)
        
        print(f"âœ“ çŸ¥è¯†åº“å·²ä¿å­˜åˆ° {filepath_prefix}")
    
    def load_from_file(self, filepath_prefix: str):
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“"""
        # åŠ è½½å…ƒæ•°æ®
        with open(f"{filepath_prefix}_metadata.json", 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # æ¢å¤æ•°æ®
        self.pairs = [TranslationPair(**pair_data) for pair_data in metadata['pairs']]
        self.category_index = defaultdict(list, metadata['category_index'])
        self.id_to_index = metadata['id_to_index']
        
        config = metadata['config']
        self.embedding_dim = config['embedding_dim']
        self.index_type = config['index_type']
        self.max_results = config['max_results']
        
        # é‡æ–°åˆå§‹åŒ–FAISSç´¢å¼•
        self._init_faiss_index()
        
        # åŠ è½½embeddings
        embeddings_path = f"{filepath_prefix}_embeddings.npy"
        if os.path.exists(embeddings_path):
            self.embeddings = np.load(embeddings_path)
        
        # åŠ è½½FAISSç´¢å¼•
        index_path = f"{filepath_prefix}.index"
        if os.path.exists(index_path):
            self.faiss_index = faiss.read_index(index_path)
            self.is_built = True
        else:
            self.is_built = False
        
        print(f"âœ“ çŸ¥è¯†åº“å·²ä» {filepath_prefix} åŠ è½½")
    
    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_pairs': len(self.pairs),
            'categories': {cat: len(indices) for cat, indices in self.category_index.items()},
            'index_type': self.index_type,
            'embedding_dim': self.embedding_dim,
            'is_built': self.is_built,
            'faiss_index_size': self.faiss_index.ntotal if self.is_built else 0
        }
        
        if self.embeddings is not None:
            stats['embeddings_shape'] = self.embeddings.shape
        
        return stats